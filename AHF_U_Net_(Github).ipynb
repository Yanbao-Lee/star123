{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanbao-Lee/star123/blob/main/AHF_U_Net_(Github).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WUfDyTBNKwRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7bf723-f0b2-41c4-a9d0-7a6eea2b858f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "IMAGE_DATASET_PATH = \"/content/drive/MyDrive/ISIC2016/images\"\n",
        "MASK_DATASET_PATH = \"/content/drive/MyDrive/ISIC2016/masks\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Images path:\", IMAGE_DATASET_PATH)\n",
        "print(\"Masks path:\", MASK_DATASET_PATH)\n",
        "\n",
        "print(\"Number of image files:\", len(os.listdir(IMAGE_DATASET_PATH)))\n",
        "print(\"Number of mask files:\", len(os.listdir(MASK_DATASET_PATH)))"
      ],
      "metadata": {
        "id": "rVDzR0BYlJKd",
        "outputId": "565e82fa-cb1e-4ee3-8646-362d4b4327e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images path: /content/drive/MyDrive/ISIC2016/images\n",
            "Masks path: /content/drive/MyDrive/ISIC2016/masks\n",
            "Number of image files: 901\n",
            "Number of mask files: 901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I4DFfcYDK0rP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#import torchvision.transforms as transforms\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import CenterCrop\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import cv2\n",
        "from torch.nn import Parameter\n",
        "# torch.manual_seed(42)\n",
        "# np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-xhMX1pnK0ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2240dcf-a000-4abb-ce01-45a542925446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "IMAGE_DATASET_PATH = '/content/drive/MyDrive/Phd work/ISIC 2017/ISIC_2017/'\n",
        "MASK_DATASET_PATH = '/content/drive/MyDrive/Phd work/ISIC 2017/ISIC-2017_Training_Part1_GroundTruth/'\n",
        "\n",
        "INPUT_IMAGE_HEIGHT = 240  # 352 in original paper\n",
        "INPUT_IMAGE_WIDTH = 240\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "seed = np.random.randint(2147483647)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "TEST_SPLIT = 0.30\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lFLo4gVUK0wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe74472-e187-425c-d10f-4b512da77669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 630\n",
            "Test images: 270\n",
            "Train masks: 630\n",
            "Test masks: 270\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def list_images(directory):\n",
        "    valid_exts = ['.jpg', '.jpeg', '.png']\n",
        "    return sorted([os.path.join(directory, f) for f in os.listdir(directory)\n",
        "                   if os.path.splitext(f.lower())[1] in valid_exts])\n",
        "\n",
        "IMAGE_DATASET_PATH = \"/content/drive/MyDrive/ISIC2016/images\"\n",
        "MASK_DATASET_PATH = \"/content/drive/MyDrive/ISIC2016/masks\"\n",
        "\n",
        "imagePaths = list_images(IMAGE_DATASET_PATH)\n",
        "maskPaths = list_images(MASK_DATASET_PATH)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seed = 42  # 也可以用你的随机种子\n",
        "TEST_SPLIT = 0.3\n",
        "\n",
        "trainImages, testImages, trainMasks, testMasks = train_test_split(\n",
        "    imagePaths, maskPaths, test_size=TEST_SPLIT, random_state=seed)\n",
        "\n",
        "print(f\"Train images: {len(trainImages)}\")\n",
        "print(f\"Test images: {len(testImages)}\")\n",
        "print(f\"Train masks: {len(trainMasks)}\")\n",
        "print(f\"Test masks: {len(testMasks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LxX3kdfRK0zL"
      },
      "outputs": [],
      "source": [
        "#create dataset class\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "class SegmentationDataset(Dataset):\n",
        "\tdef __init__(self, imagePaths, maskPaths, transforms):\n",
        "\t\t# store the image and mask filepaths, and augmentation\n",
        "\t\t# transforms\n",
        "\t\tself.imagePaths = imagePaths\n",
        "\t\tself.maskPaths = maskPaths\n",
        "\t\tself.transforms = transforms\n",
        "\tdef __len__(self):\n",
        "\t\t# return the number of total samples contained in the dataset\n",
        "\t\treturn len(self.imagePaths)\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\t# grab the image path from the current index\n",
        "\t\timagePath = self.imagePaths[idx]\n",
        "\t\timage = cv2.imread(imagePath)\n",
        "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\t\tmask = cv2.imread(self.maskPaths[idx], 0)\n",
        "\n",
        "\t\tif self.transforms is not None:\n",
        "\t\t\t# apply the transformations to both image and its mask\n",
        "\t\t\timage = self.transforms(image)\n",
        "\t\t\tmask = self.transforms(mask)\n",
        "\t\t# return a tuple of the image and its mask\n",
        "\t\treturn (image, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uXbGbXoBK01r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f0f42b-15de-4260-aa1b-f22344194972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] found 630 examples in the training set...\n",
            "[INFO] found 270 examples in the test set...\n"
          ]
        }
      ],
      "source": [
        "# define transformations\n",
        "from PIL import Image\n",
        "transforms1 = transforms.Compose([transforms.ToPILImage(),\n",
        " \ttransforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH),interpolation= Image.NEAREST),\n",
        "\ttransforms.ToTensor()])\n",
        "\n",
        "# create the train and test datasets\n",
        "trainDS = SegmentationDataset(imagePaths=trainImages, maskPaths=trainMasks,transforms=transforms1)\n",
        "testDS = SegmentationDataset(imagePaths=testImages, maskPaths=testMasks,transforms=transforms1)\n",
        "\n",
        "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
        "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
        "\n",
        "# create the training and test data loaders\n",
        "trainLoader = DataLoader(trainDS, shuffle=True, batch_size= BATCH_SIZE, num_workers=os.cpu_count()) #pin_memory= PIN_MEMORY\n",
        "testLoader = DataLoader(testDS, shuffle=False, batch_size= BATCH_SIZE, num_workers=os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGmDHNkoK04p"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# obtain a batch of training data\n",
        "inputs, masks = next(iter(trainLoader))\n",
        "\n",
        "print(\"Shape of the inputs tensor:\", inputs.shape)\n",
        "print(\"Shape of the masks tensor:\", masks.shape)\n",
        "\n",
        "# plot the images and masks from the batch\n",
        "fig, axs = plt.subplots(4, 2, figsize=(12, 6))\n",
        "for i in range(4):\n",
        "    for j in range(2):\n",
        "        idx = i*2+j\n",
        "        # extract the image and mask tensors from the batch\n",
        "        img_tensor = inputs[idx]\n",
        "        mask_tensor = masks[idx]\n",
        "        # convert the tensors to numpy arrays and transpose the dimensions\n",
        "        img = img_tensor.permute(1, 2, 0).numpy()\n",
        "        mask = mask_tensor.squeeze().numpy()\n",
        "        # plot the image and mask side-by-side\n",
        "        axs[i, j].imshow(img)\n",
        "        axs[i, j].imshow(mask,alpha = 0.5)\n",
        "        axs[i, j].axis('off')\n",
        "plt.show()\n",
        "\n",
        "unique_values = masks.unique()\n",
        "print(\"Unique values in the mask tensor:\", unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH7jrbKHqxZ6"
      },
      "outputs": [],
      "source": [
        "class AFF(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, channels, r=4):\n",
        "        super(AFF, self).__init__()\n",
        "        inter_channels = int(channels // r)\n",
        "\n",
        "        self.local_att = nn.Sequential(\n",
        "            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(inter_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(channels),\n",
        "        )\n",
        "\n",
        "        self.global_att = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(inter_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(channels),\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x, outI, feature_Level):\n",
        "        xa = x + outI + feature_Level # x = I_out4, outI = x5,  feature_Level = x4\n",
        "        #print(\"Inside AFF xa: \", xa.size())\n",
        "        xl = self.local_att(xa)\n",
        "        #print(\"Inside AFF xl: \", xl.size())\n",
        "        xg = self.global_att(xa)\n",
        "        #print(\"Inside AFF xg: \", xg.size())\n",
        "        xlg = xl + xg\n",
        "        #print(\"Inside AFF xlg: \", xlg.size())\n",
        "        wei = self.sigmoid(xlg)\n",
        "\n",
        "        xo = 2 * x * wei + 2 * outI * (1 - wei)\n",
        "        #print(\"Inside AFF xo: \", wei.size())\n",
        "        return xo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckfrm2OIAi2U"
      },
      "outputs": [],
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(Model1, self).__init__()\n",
        "\n",
        "        #for single conv\n",
        "        self.conv1_single = nn.Conv2d(1, 1, kernel_size=1, padding=0)\n",
        "        self.conv_single = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.spatial_maxpool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.spatial_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\n",
        "    def forward(self, F1, F2, F3):\n",
        "\n",
        "        FRi = F1 + F2 + F3 # F1 = I_out4, F2 = x5, F3 = x4\n",
        "\n",
        "        channel_maxpool = torch.max(FRi, dim=1, keepdim=True)[0] # maxpool return two variable(value, index)\n",
        "        channel_avgpool = torch.mean(FRi, dim=1, keepdim=True)\n",
        "\n",
        "        xl = self.relu(self.conv_single(FRi)) * FRi\n",
        "        #print(\"local features: \", xl.shape)\n",
        "\n",
        "        # # Global Average Pooling (spatial-wise pooling, channel will be same)\n",
        "        GAP = self.relu(self.conv_single(self.spatial_avgpool(FRi)))* FRi\n",
        "        #print(\"GAP: \", GAP.shape)\n",
        "\n",
        "\n",
        "        # for single conv\n",
        "        #GMP = self.sigmoid(self.conv_single(self.spatial_maxpool(FRi)))* FRi\n",
        "        GMP = self.relu(self.conv_single(self.spatial_maxpool(FRi)))* FRi\n",
        "        #print(\"GMP: \", GMP.shape)\n",
        "\n",
        "\n",
        "        # for single conv\n",
        "        #MP = self.sigmoid(self.conv1_single(channel_maxpool)) * FRi\n",
        "        MP = self.relu(self.conv1_single(channel_maxpool)) * FRi\n",
        "        #print(\"MP: \", MP.shape)\n",
        "\n",
        "\n",
        "\n",
        "        # for single conv\n",
        "        #AP = self.sigmoid(self.conv1_single(channel_avgpool)) * FRi\n",
        "        AP = self.relu(self.conv1_single(channel_avgpool)) * FRi\n",
        "        #print(\"AP:\", AP.shape)\n",
        "\n",
        "\n",
        "        addition = GAP + xl + AP\n",
        "\n",
        "        addition = self.sigmoid(addition)\n",
        "        #print(\"Output: \", addition. shape)\n",
        "\n",
        "        #xo = 2 * F1 * addition + 2 * F3 * (1 - addition)\n",
        "        xo = 2 * F1 * addition + 2 * F2 * (1 - addition)\n",
        "        #print(\"xo: \", xo. shape)\n",
        "\n",
        "        return addition\n",
        "#Sample usage\n",
        "# fem = Model1(channels=64)\n",
        "# input_tensor1 = torch.randn(4, 64, 240, 240)\n",
        "# input_tensor2 = torch.randn(4, 64, 240, 240)\n",
        "# output_tensor = fem(input_tensor1,input_tensor2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf32Z6C70wG-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, AFF_channel, bilinear=False):\n",
        "    #def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        #self.model1 = Model1(AFF_channel)\n",
        "        self.aff= AFF(AFF_channel)\n",
        "        #self.iaff = iAFF(AFF_channel)\n",
        "\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels) #AFF\n",
        "\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels//2, out_channels)\n",
        "\n",
        "\n",
        "    #def forward(self, x1, x2):\n",
        "    def forward(self, x1, x2, x3):\n",
        "        #print(\"inside forward up( initial call) x1 and x2: \", x1.size(),x2.size())\n",
        "        x1 = self.up(x1) # during first call x1 = x5, x2 = x4 (x1, x2, x3 == x5, I_out_4, x4)\n",
        "        #print(\"inside after calling up, x1=x5: \", x1.size())\n",
        "        #input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        #x = self.model1(x2,x1,x3)\n",
        "        x = self.aff(x2,x1,x3)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "       return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jO-F8ZPoU2Z"
      },
      "outputs": [],
      "source": [
        "#class CLSCA(nn.Module):\n",
        "class UAFF(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False, dropout_rate=0.0):\n",
        "        #super(CLSCA, self).__init__()\n",
        "        super(UAFF, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.spatial_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        #channel_maxpool = torch.max(FRi, dim=1, keepdim=True)[0]\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "           )\n",
        "\n",
        "        self.trans_conv1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "           )\n",
        "\n",
        "        self.trans_conv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "           )\n",
        "\n",
        "        self.trans_conv3 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "           )\n",
        "\n",
        "        self.trans_conv4 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "\n",
        "        \"\"\" For AFF and iAFF\"\"\"\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        #factor = 2 if bilinear else 1\n",
        "        factor = 2\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512 , 512)\n",
        "        self.up2 = Up(512, 256, 256)\n",
        "        self.up3 = Up(256, 128 , 128)\n",
        "        self.up4 = Up(128, 64, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        s1 = self.spatial_avgpool(x1)\n",
        "        c1 = torch.mean(x1, dim=1, keepdim=True)\n",
        "        mul1 = s1 * c1\n",
        "\n",
        "        x2 = self.down1(x1)\n",
        "        s2 = self.spatial_avgpool(x2)\n",
        "        c2 = torch.mean(x2, dim=1, keepdim=True)\n",
        "        mul2 = s2 * c2\n",
        "\n",
        "        Trans_conv1 = self.trans_conv1(mul2)\n",
        "        I_out_1 = self.conv1(mul1 + Trans_conv1)\n",
        "        I_out_1 = I_out_1 * x1\n",
        "\n",
        "        x3 = self.down2(x2)\n",
        "        s3 = self.spatial_avgpool(x3)\n",
        "        c3 = torch.mean(x3, dim=1, keepdim=True)\n",
        "        mul3 = s3 * c3\n",
        "\n",
        "        Trans_conv2 = self.trans_conv2(mul3)\n",
        "        I_out_2 = self.conv2(mul2 + Trans_conv2)\n",
        "        I_out_2 =  I_out_2 * x2\n",
        "        x4 = self.down3(x3)\n",
        "        s4 = self.spatial_avgpool(x4)\n",
        "        c4 = torch.mean(x4, dim=1, keepdim=True)\n",
        "        mul4 = s4 * c4\n",
        "        Trans_conv3 = self.trans_conv3(mul4)\n",
        "        I_out_3 = self.conv3(mul3 + Trans_conv3)\n",
        "        I_out_3 =  I_out_3 * x3\n",
        "\n",
        "        x5 = self.down4(x4)\n",
        "        s5 = self.spatial_avgpool(x5)\n",
        "        c5 = torch.mean(x5, dim=1, keepdim=True)\n",
        "        mul5 = s5 * c5\n",
        "\n",
        "        Trans_conv4 = self.trans_conv4(mul5)\n",
        "        I_out_4 = self.conv4(mul4 + Trans_conv4)\n",
        "        I_out_4 = I_out_4 * x4\n",
        "\n",
        "\n",
        "        x = self.up1(x5, I_out_4, x4)\n",
        "        x = self.up2(x, I_out_3, x3)\n",
        "        x = self.up3(x, I_out_2, x2)\n",
        "        x = self.up4(x, I_out_1, x1)\n",
        "        logits = self.outc(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pxza_kXK1Hz"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from tqdm import tqdm\n",
        "criterion = BCEWithLogitsLoss()\n",
        "NUM_EPOCHS = 70\n",
        "trainSteps = len(trainDS) // BATCH_SIZE\n",
        "testSteps = len(testDS) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKpKxuRCDpQQ"
      },
      "outputs": [],
      "source": [
        "def evaluation_metrices(output, target):\n",
        "  smooth = 1e-5\n",
        "\n",
        "  if torch.is_tensor(output):\n",
        "    output = torch.sigmoid(output).data.cpu().numpy()\n",
        "  if torch.is_tensor(target):\n",
        "    target = target.data.cpu().numpy()\n",
        "\n",
        "    output_ = output > 0.5\n",
        "    target_ = target > 0.5\n",
        "\n",
        "    true_positives = (output_ & target_).sum()\n",
        "    false_positives = (output_ & ~target_).sum()\n",
        "    false_negatives = (~output_ & target_).sum()\n",
        "    true_negatives = (~output_ & ~target_).sum()\n",
        "\n",
        "    sensitivity = (true_positives ) #/ (true_positives + false_negatives )\n",
        "    specificity = (true_negatives) #/ (true_negatives + false_positives )\n",
        "    accuracy = (true_positives + true_negatives ) #/ (true_positives + true_negatives + false_positives + false_negatives )\n",
        "\n",
        "\n",
        "    intersection = true_positives\n",
        "    union = true_positives + false_positives + false_negatives\n",
        "\n",
        "    iou = intersection  #/ (union + smooth)\n",
        "    dice = 2 * intersection  #/ (union + intersection + smooth)\n",
        "\n",
        "    #iou = (intersection + smooth) / (union + smooth)\n",
        "    #dice = (2 * intersection + smooth) / (union + intersection + smooth)\n",
        "\n",
        "    return iou, dice, accuracy, sensitivity, specificity, true_positives, false_positives, false_negatives, true_negatives\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mb06GPe1Klr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ebee6c-e96b-42e3-963d-6ddacfc53357"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "moving models to GPU ...\n",
            "done\n",
            "\n",
            "[INFO] training the network...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/70 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] EPOCH: 1/70\n",
            "Train loss: 0.4878, Val loss: 0.3814, Train_iou: 0.5213, Val_iou: 0.6330\n",
            "Dice Score: 0.7752, Accuracy: 0.8761, Sensitivity: 0.8221, Specificity: 0.8951\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|▏         | 1/70 [06:22<7:19:26, 382.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved best model based on IOU:  0.6329555184082848\n",
            "[INFO] EPOCH: 2/70\n",
            "Train loss: 0.3695, Val loss: 0.3322, Train_iou: 0.6170, Val_iou: 0.6629\n",
            "Dice Score: 0.7973, Accuracy: 0.9023, Sensitivity: 0.7397, Specificity: 0.9594\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 2/70 [07:29<3:43:20, 197.06s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved best model based on IOU:  0.6629272166742329\n",
            "[INFO] EPOCH: 3/70\n",
            "Train loss: 0.3165, Val loss: 0.2858, Train_iou: 0.6588, Val_iou: 0.7036\n",
            "Dice Score: 0.8260, Accuracy: 0.9171, Sensitivity: 0.7571, Specificity: 0.9733\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 3/70 [08:38<2:34:27, 138.33s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved best model based on IOU:  0.7036452597869156\n",
            "[INFO] EPOCH: 4/70\n",
            "Train loss: 0.2937, Val loss: 0.2560, Train_iou: 0.6718, Val_iou: 0.7060\n",
            "Dice Score: 0.8277, Accuracy: 0.9173, Sensitivity: 0.7639, Specificity: 0.9712\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 4/70 [09:47<2:02:06, 111.01s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved best model based on IOU:  0.7059920245393161\n",
            "[INFO] EPOCH: 5/70\n",
            "Train loss: 0.2784, Val loss: 0.2417, Train_iou: 0.6706, Val_iou: 0.7248\n",
            "Dice Score: 0.8405, Accuracy: 0.9199, Sensitivity: 0.8122, Specificity: 0.9577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 5/70 [10:56<1:44:00, 96.01s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved best model based on IOU:  0.7248285702234356\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▊         | 6/70 [12:03<1:31:58, 86.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] EPOCH: 6/70\n",
            "Train loss: 0.2512, Val loss: 0.2393, Train_iou: 0.6997, Val_iou: 0.7009\n",
            "Dice Score: 0.8241, Accuracy: 0.9186, Sensitivity: 0.7340, Specificity: 0.9834\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[INFO] EPOCH: 7/70\n",
            "Train loss: 0.2354, Val loss: 0.1974, Train_iou: 0.7178, Val_iou: 0.7470\n",
            "Dice Score: 0.8552, Accuracy: 0.9293, Sensitivity: 0.8029, Specificity: 0.9738\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 7/70 [13:09<1:23:37, 79.64s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved best model based on IOU:  0.7470142798365375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█▏        | 8/70 [14:17<1:18:24, 75.88s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] EPOCH: 8/70\n",
            "Train loss: 0.2143, Val loss: 0.2033, Train_iou: 0.7383, Val_iou: 0.7343\n",
            "Dice Score: 0.8468, Accuracy: 0.9245, Sensitivity: 0.8030, Specificity: 0.9672\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[INFO] EPOCH: 9/70\n",
            "Train loss: 0.2043, Val loss: 0.1784, Train_iou: 0.7469, Val_iou: 0.7622\n",
            "Dice Score: 0.8651, Accuracy: 0.9351, Sensitivity: 0.8010, Specificity: 0.9821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 9/70 [15:24<1:14:04, 72.87s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved best model based on IOU:  0.7622067856759399\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 10/70 [16:31<1:11:19, 71.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] EPOCH: 10/70\n",
            "Train loss: 0.1909, Val loss: 0.1969, Train_iou: 0.7595, Val_iou: 0.7474\n",
            "Dice Score: 0.8554, Accuracy: 0.9309, Sensitivity: 0.7870, Specificity: 0.9814\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[INFO] EPOCH: 11/70\n",
            "Train loss: 0.1784, Val loss: 0.1791, Train_iou: 0.7773, Val_iou: 0.7740\n",
            "Dice Score: 0.8726, Accuracy: 0.9383, Sensitivity: 0.8137, Specificity: 0.9820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 11/70 [17:36<1:08:08, 69.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved best model based on IOU:  0.7739967733294542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 12/70 [18:47<1:07:21, 69.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] EPOCH: 12/70\n",
            "Train loss: 0.1815, Val loss: 0.1934, Train_iou: 0.7711, Val_iou: 0.7564\n",
            "Dice Score: 0.8613, Accuracy: 0.9294, Sensitivity: 0.8439, Specificity: 0.9594\n",
            "EarlyStopping counter: 1 out of 20\n"
          ]
        }
      ],
      "source": [
        "# my training loop\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "H = {\"train_loss\": [], \"train_iou\": [], \"val_loss\": [],\"val_iou\": [], \"dice_score\": [], \"accuracy\": [], \"sensitivity\": [], \"specificity\": []}\n",
        "\n",
        "def main():\n",
        "\n",
        "    model = UAFF(3,1,bilinear=False)\n",
        "\n",
        "    # move to GPU\n",
        "    print('\\nmoving models to GPU ...')\n",
        "    ##model = torch.nn.DataParallel(model).cuda()\n",
        "    model.cuda()\n",
        "    criterion.to(DEVICE)\n",
        "    print('done\\n')\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2.0e-4, weight_decay=0)\n",
        "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "    print(\"[INFO] training the network...\")\n",
        "\n",
        "    startTime = time.time()\n",
        "\n",
        "    best_iou = 0\n",
        "    trigger = 0\n",
        "    patience = 20\n",
        "\n",
        "    # 你自己设定这些变量，否则会报错\n",
        "    # NUM_EPOCHS, trainLoader, testLoader, trainSteps, testSteps, testDS\n",
        "\n",
        "    # 确保保存路径存在\n",
        "    save_model_dir = \"/content/drive/MyDrive/ISIC2016/Saved_model/u-net/UAFF-New-Save\"\n",
        "    os.makedirs(save_model_dir, exist_ok=True)\n",
        "\n",
        "    results_dir = \"/content/drive/MyDrive/ISIC2016/Paper_Results\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    for e in tqdm(range(NUM_EPOCHS)):\n",
        "        train_loss, train_iou, train_iou_Deno = train_function(trainLoader, model, criterion, optimizer)\n",
        "        totalValLoss, val_iou, val_dice, val_accuracy,val_sensitivity,val_specificity, val_sen_Deno, val_spc_Deno,val_iou_Deno, val_dice_Deno = val_function(testLoader, model, criterion, optimizer)\n",
        "\n",
        "        avgTrainLoss = train_loss / trainSteps\n",
        "        avgTrainIoU = train_iou/ train_iou_Deno\n",
        "        avgValLoss = totalValLoss / testSteps\n",
        "        avgValIoU = val_iou / val_iou_Deno\n",
        "        avgValDice = val_dice/ val_dice_Deno\n",
        "        avgValAcc = val_accuracy/ (len(testDS)*(240*240))\n",
        "        avgValSen = val_sensitivity/ val_sen_Deno\n",
        "        avgValSpe = val_specificity/ val_spc_Deno\n",
        "\n",
        "        print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
        "        print(\"Train loss: {:.4f}, Val loss: {:.4f}, Train_iou: {:.4f}, Val_iou: {:.4f}\".format(avgTrainLoss, avgValLoss, avgTrainIoU,avgValIoU))\n",
        "        print(\"Dice Score: {:.4f}, Accuracy: {:.4f}, Sensitivity: {:.4f}, Specificity: {:.4f}\".format(avgValDice,avgValAcc,avgValSen,avgValSpe))\n",
        "\n",
        "        if avgValIoU > best_iou:\n",
        "            torch.save(model.state_dict(), os.path.join(save_model_dir, \"ISIC.pth\"))\n",
        "            best_iou=avgValIoU\n",
        "            print(\"saved best model based on IOU: \", best_iou )\n",
        "            trigger = 0\n",
        "        else:\n",
        "            trigger += 1\n",
        "            print(f'EarlyStopping counter: {trigger} out of {patience}')\n",
        "\n",
        "        if trigger >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        if (e % 1 == 0):\n",
        "            H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "            H[\"train_iou\"].append(avgTrainIoU)\n",
        "            H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "            H[\"val_iou\"].append(avgValIoU)\n",
        "            H[\"dice_score\"].append(avgValDice)\n",
        "            H[\"accuracy\"].append(avgValAcc)\n",
        "            H[\"sensitivity\"].append(avgValSen)\n",
        "            H[\"specificity\"].append(avgValSpe)\n",
        "\n",
        "    endTime = time.time()\n",
        "    print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
        "\n",
        "    df = pd.DataFrame(H)\n",
        "    df.to_csv(os.path.join(save_model_dir, \"ISIC.csv\"), index=False)\n",
        "\n",
        "    epochs_range = range(1, len(H['train_iou'])+1)\n",
        "\n",
        "    plt.plot(epochs_range, H['train_iou'], label='train_iou')\n",
        "    plt.plot(epochs_range, H['val_iou'], label='val_iou')\n",
        "    plt.title('IOU vs Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('IOU')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, \"UNet.png\"))\n",
        "    plt.show()\n",
        "    time.sleep(4)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def train_function(trainLoader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    totalTrainLoss = 0\n",
        "    train_iou = 0\n",
        "    train_iou_Deno = 0\n",
        "\n",
        "    for (i, (x, y)) in enumerate(trainLoader):\n",
        "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        totalTrainLoss += loss\n",
        "        iou, dice, accuracy, sensitivity, specificity, true_positives, false_positives, \\\n",
        "        false_negatives, true_negatives = evaluation_metrices(output,y)\n",
        "        train_iou += iou\n",
        "        train_iou_Deno += true_positives + false_positives + false_negatives\n",
        "    return totalTrainLoss, train_iou, train_iou_Deno\n",
        "\n",
        "\n",
        "def val_function(testLoader, model, criterion, optimizer):\n",
        "    totalValLoss = 0\n",
        "    val_iou = 0\n",
        "    val_dice = 0\n",
        "    val_accuracy = 0\n",
        "    val_sensitivity = 0\n",
        "    val_specificity = 0\n",
        "    val_sen_Deno = 0\n",
        "    val_spc_Deno = 0\n",
        "    val_iou_Deno = 0\n",
        "    val_dice_Deno = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for (x,y) in testLoader:\n",
        "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "            output = model(x)\n",
        "            ValLoss = criterion(output, y)\n",
        "            totalValLoss += ValLoss\n",
        "            iou, dice, accuracy, sensitivity, specificity, true_positives,\\\n",
        "            false_positives, false_negatives, true_negatives = evaluation_metrices(output,y)\n",
        "            val_iou += iou\n",
        "            val_dice += dice\n",
        "            val_accuracy += accuracy\n",
        "            val_sensitivity += sensitivity\n",
        "            val_specificity += specificity\n",
        "            val_sen_Deno += true_positives + false_negatives\n",
        "            val_spc_Deno += true_negatives + false_positives\n",
        "            val_iou_Deno += true_positives + false_positives + false_negatives\n",
        "            val_dice_Deno += 2*true_positives + false_positives + false_negatives\n",
        "    return totalValLoss, val_iou, val_dice, val_accuracy, val_sensitivity, val_specificity, val_sen_Deno, val_spc_Deno, val_iou_Deno, val_dice_Deno\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAjP_0Zn0sAr"
      },
      "outputs": [],
      "source": [
        "# Use this if you need to run multiple time\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o-oByZvoiUl"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import optuna\n",
        "# from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# db_url = \"sqlite:////content/gdrive/MyDrive/Phd work/SAVE_SQL/test.db\"\n",
        "\n",
        "# def objective(trial):\n",
        "#     # Define hyperparameters to be optimized\n",
        "#     lr = trial.suggest_float(\"lr\", 2e-5, 1e-3, log=True)\n",
        "\n",
        "#     weight_decay = trial.suggest_float(\"weight_decay\", 2e-5, 1e-3, log=True)\n",
        "#     dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
        "\n",
        "#     # Learning rate scheduler parameters\n",
        "#     step_size = trial.suggest_int('step_size', 1, 10)\n",
        "#     gamma = trial.suggest_float('gamma', 0.1, 1.0)\n",
        "\n",
        "\n",
        "#     IoU = main(lr,weight_decay,dropout_rate, step_size, gamma)\n",
        "\n",
        "#     return IoU\n",
        "\n",
        "\n",
        "\n",
        "# H = {\"train_loss\": [], \"train_iou\": [], \"val_loss\": [],\"val_iou\": [], \"dice_score\": [], \"accuracy\": [], \"sensitivity\": [], \"specificity\": []}\n",
        "\n",
        "\n",
        "# def main(lr=1e-4, weight_decay=0, dropout_rate = .1, step_size=5, gamma=.1):\n",
        "\n",
        "#     model = UNetAFF(3,1,bilinear=False,dropout_rate=dropout_rate)\n",
        "\n",
        "#     print('\\nmoving models to GPU ...')\n",
        "#     model.cuda()\n",
        "#     criterion.to(DEVICE)\n",
        "#     print('done\\n')\n",
        "\n",
        "#    # Select optimizer based on trial suggestion\n",
        "\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "#     startTime = time.time()\n",
        "\n",
        "#     best_iou = 0\n",
        "#     trigger = 0\n",
        "#     patience = 20\n",
        "\n",
        "#     for e in tqdm(range(NUM_EPOCHS)):\n",
        "# \t  # set the model in training mode\n",
        "#           train_loss, train_iou, train_iou_Deno = train_function(trainLoader, model, criterion, optimizer)\n",
        "#           totalValLoss, val_iou, val_dice, val_accuracy,val_sensitivity,val_specificity, val_sen_Deno, val_spc_Deno,val_iou_Deno, val_dice_Deno = val_function(testLoader, model, criterion, optimizer)\n",
        "#           scheduler.step()\n",
        "\n",
        "# \t# calculate the average training and validation loss\n",
        "#           avgTrainLoss = train_loss / trainSteps\n",
        "#           avgTrainIoU = train_iou/ train_iou_Deno\n",
        "\n",
        "#           avgValLoss = totalValLoss / testSteps\n",
        "#           avgValIoU = val_iou / val_iou_Deno\n",
        "#           avgValDice = val_dice/ val_dice_Deno\n",
        "#           avgValAcc = val_accuracy/ (len(testDS)*(240*240))\n",
        "#           avgValSen = val_sensitivity/ val_sen_Deno\n",
        "#           avgValSpe = val_specificity/ val_spc_Deno\n",
        "\n",
        "#           print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
        "#           print(\"Train loss: {:.4f}, Val loss: {:.4f}, Train_iou: {:.4f}, Val_iou: {:.4f}\".format(avgTrainLoss, avgValLoss, avgTrainIoU,avgValIoU))\n",
        "#           print(\"Dice Score: {:.4f}, Accuracy: {:.4f}, Sensitivity: {:.4f}, Specificity: {:.4f}\".format(avgValDice,avgValAcc,avgValSen,avgValSpe))\n",
        "\n",
        "#           if avgValIoU > best_iou:\n",
        "\n",
        "#              torch.save(model.state_dict(),'/content/gdrive/MyDrive/Phd work/UNet++/UNetAFF.pth')\n",
        "\n",
        "#              best_iou=avgValIoU\n",
        "#              print(\"saved best model based on IOU: \", best_iou )\n",
        "#              trigger = 0\n",
        "#           else:\n",
        "#              trigger += 1\n",
        "#              print(f'EarlyStopping counter: {trigger} out of {patience}')\n",
        "\n",
        "#           if trigger >= patience:\n",
        "#                   #torch.save(model.state_dict(),'/content/gdrive/MyDrive/Phd work/UNet++/UNetAFF.pth')\n",
        "#                   print(\"Early stopping triggered\")\n",
        "#                   break\n",
        "\n",
        "# \t# update our training history\n",
        "#           if (e%1==0):\n",
        "#               H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "#               H[\"train_iou\"].append(avgTrainIoU) # The cpu() method is used to move the tensor from GPU to CPU (if it is on the GPU)\n",
        "#                                                                       # because numpy arrays can only be created from tensors on CPU.\n",
        "#               H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "#               H[\"val_iou\"].append(avgValIoU)\n",
        "#               H[\"dice_score\"].append(avgValDice)\n",
        "#               H[\"accuracy\"].append(avgValAcc)\n",
        "#               H[\"sensitivity\"].append(avgValSen)\n",
        "#               H[\"specificity\"].append(avgValSpe)\n",
        "\n",
        "\n",
        "#     endTime = time.time()\n",
        "#     print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
        "\n",
        "#     df = pd.DataFrame(H)\n",
        "\n",
        "#     df.to_csv('/content/gdrive/MyDrive/Phd work/Paper_Results/UNet_Updated.csv', index=False)\n",
        "\n",
        "#     epochs_range = range(1, len(H['train_iou'])+1)\n",
        "\n",
        "#     plt.plot(epochs_range, H['train_iou'], label='train_iou')\n",
        "#     plt.plot(epochs_range, H['val_iou'], label='val_iou')\n",
        "#     plt.title('IOU vs Epoch')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('IOU')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "#     plt.tight_layout()\n",
        "#     time.sleep(4)\n",
        "\n",
        "#     plt.savefig('/content/gdrive/MyDrive/Phd work/Paper_Results/UNet.png')\n",
        "\n",
        "\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     return avgValIoU\n",
        "\n",
        "\n",
        "# def train_function(trainLoader, model, criterion, optimizer):\n",
        "#           model.train()\n",
        "#           totalTrainLoss = 0\n",
        "#           train_iou = 0\n",
        "#           train_iou_Deno = 0\n",
        "\n",
        "# \t # loop over the training set\n",
        "#           for (i, (x, y)) in enumerate(trainLoader):\n",
        "# \t\t# send the input to the device\n",
        "#               (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "#               #torch.cuda.empty_cache()\n",
        "#               #gc.collect()\n",
        "#               output = model(x) #output is the raw output from the model\n",
        "#               loss = criterion(output, y)\n",
        "#               optimizer.zero_grad()\n",
        "#               loss.backward()\n",
        "#               optimizer.step()\n",
        "#               totalTrainLoss += loss\n",
        "#               #pred = output.argmax(dim=1)\n",
        "#               iou, dice, accuracy, sensitivity, specificity, true_positives, false_positives, \\\n",
        "#               false_negatives, true_negatives = evaluation_metrices(output,y)\n",
        "#               train_iou += iou\n",
        "#               train_iou_Deno += true_positives + false_positives + false_negatives\n",
        "#           return totalTrainLoss, train_iou, train_iou_Deno\n",
        "\n",
        "# def val_function(testLoader, model, criterion, optimizer):\n",
        "#        totalValLoss = 0\n",
        "#        val_iou = 0\n",
        "#        val_dice = 0\n",
        "#        val_accuracy = 0\n",
        "#        val_sensitivity = 0\n",
        "#        val_specificity = 0\n",
        "#        val_sen_Deno = 0\n",
        "#        val_spc_Deno = 0\n",
        "#        val_iou_Deno = 0\n",
        "#        val_dice_Deno = 0\n",
        "\n",
        "#        with torch.no_grad():\n",
        "\n",
        "#               model.eval() # set the model in evaluation mode\n",
        "\n",
        "#               for (x,y) in testLoader:\n",
        "#                   (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "#                   output = model(x)\n",
        "#                   ValLoss = criterion(output, y)\n",
        "#                   totalValLoss += ValLoss\n",
        "#                   iou, dice, accuracy, sensitivity, specificity, true_positives,\\\n",
        "#                   false_positives, false_negatives, true_negatives = evaluation_metrices(output,y)\n",
        "#                   val_iou += iou\n",
        "#                   val_dice += dice\n",
        "#                   val_accuracy +=accuracy\n",
        "#                   val_sensitivity += sensitivity\n",
        "#                   val_specificity += specificity\n",
        "#                   val_sen_Deno += true_positives + false_negatives\n",
        "#                   val_spc_Deno += true_negatives + false_positives\n",
        "#                   val_iou_Deno += true_positives + false_positives + false_negatives\n",
        "#                   val_dice_Deno += 2*true_positives + false_positives + false_negatives\n",
        "#               return totalValLoss, val_iou, val_dice, val_accuracy,val_sensitivity, val_specificity, val_sen_Deno, val_spc_Deno, val_iou_Deno, val_dice_Deno\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     #study = optuna.create_study(direction=\"maximize\")\n",
        "#     study = optuna.create_study(direction=\"maximize\", study_name=\"unet-aff-optimization\", storage=db_url, load_if_exists=True)\n",
        "\n",
        "#     study.optimize(objective, n_trials=60)\n",
        "\n",
        "#     print(\"Number of finished trials:\", len(study.trials))\n",
        "#     print(\"Best trial:\", study.best_trial.params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qdWCMFG8tz34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqaXayQRUUYJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from imutils import paths\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 先挂载Google Drive（如果还没挂载）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 评估图片和掩码的文件夹路径（请替换成你的路径）\n",
        "IMAGE_DATASET_PATH_E = '/content/drive/MyDrive/ISIC2016/Eval_images/'\n",
        "MASK_DATASET_PATH_E = '/content/drive/MyDrive/ISIC2016/Eval_masks/'\n",
        "\n",
        "# 创建文件夹，避免后续找不到路径报错\n",
        "os.makedirs(IMAGE_DATASET_PATH_E, exist_ok=True)\n",
        "os.makedirs(MASK_DATASET_PATH_E, exist_ok=True)\n",
        "\n",
        "# 载入评估集图片和掩码路径\n",
        "imagePaths_E = sorted(list(paths.list_images(IMAGE_DATASET_PATH_E)))\n",
        "maskPaths_E = sorted(list(paths.list_images(MASK_DATASET_PATH_E)))\n",
        "\n",
        "print(f\"[INFO] found {len(imagePaths_E)} evaluation images\")\n",
        "print(f\"[INFO] found {len(maskPaths_E)} evaluation masks\")\n",
        "\n",
        "# 这里SegmentationDataset是你自己实现的数据集类，传入路径和transforms\n",
        "# 你需要先定义好transforms1（比如图像预处理等）\n",
        "trainDS_E = SegmentationDataset(imagePaths=imagePaths_E, maskPaths=maskPaths_E, transforms=transforms1)\n",
        "\n",
        "# DataLoader，batch_size和num_workers可根据你机器调整\n",
        "trainLoader_E = DataLoader(trainDS_E, shuffle=False, batch_size=5, num_workers=os.cpu_count())\n",
        "\n",
        "print(f\"[INFO] evaluation dataset size: {len(trainDS_E)}\")\n",
        "print(f\"[INFO] evaluation dataloader batches: {len(trainLoader_E)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQTP94h_UUjW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ISIC2016/Saved_model/u-net/UAFF-New-Save/ISIC.pth'\n",
        "\n",
        "model = UAFF(3,1,bilinear=False)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\n",
        "for image, mask in trainLoader_E:\n",
        "    break\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "\n",
        "print(\"output B: \", output.shape)\n",
        "output = output.squeeze(1).cpu().numpy()\n",
        "print(\"output A: \", output.shape)\n",
        "\n",
        "output = output > 0.5\n",
        "\n",
        "# 如果你想保存图片结果，先定义文件夹路径\n",
        "# dir_name = '/content/drive/MyDrive/ISIC2016/Paper_Results/Evaluation_image/Final_U_Net_ISIC/'\n",
        "# os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "fig, ax = plt.subplots(5, 3, figsize=(5, 10))\n",
        "for i in range(5):\n",
        "    if i == 0:\n",
        "        ax[i][0].set_title('Image')\n",
        "        ax[i][1].set_title('GT')\n",
        "        ax[i][2].set_title('Output')\n",
        "\n",
        "    ax[i][0].imshow(image[i].squeeze().permute(1, 2, 0))\n",
        "    ax[i][1].imshow(mask[i].squeeze(), cmap='gray')\n",
        "    ax[i][2].imshow(output[i], cmap='gray')\n",
        "\n",
        "    fig.subplots_adjust(wspace=0.05, hspace=0.01)\n",
        "\n",
        "    # 如果需要保存csv文件，取消下面注释并设置dir_name\n",
        "    # flattened_image = image[i].permute(1, 2, 0).reshape(-1, 3).numpy()\n",
        "    # img_df = pd.DataFrame(flattened_image)\n",
        "    # mask_df = pd.DataFrame(mask[i].numpy().squeeze(0))\n",
        "    # out_df = pd.DataFrame(output[i])\n",
        "\n",
        "    # img_df.to_csv(os.path.join(dir_name, f'img{i}.csv'), index=False)\n",
        "    # mask_df.to_csv(os.path.join(dir_name, f'mask{i}.csv'), index=False)\n",
        "    # out_df.to_csv(os.path.join(dir_name, f'output{i}.csv'), index=False)\n",
        "\n",
        "    for a in ax[i]:\n",
        "        a.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwlWlfD3OZRJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}